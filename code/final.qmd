---
title: "ENVS 193DS Final"
author: "Amit Goldberg"
date: "2025-06-09"
format: html
  toc: true
  toc-depth: 5
  number-sections: true
  theme: flatly
  code-fold: true
---

\[View the GitHub repository\] (https://github.com/amitgold11/ENVS-193DS_spring-2025_final.git)

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(glmmTMB)
library(DHARMa)
library(MuMIn)
```

# Problem 1. Research Writing

## a. Transparent Statistical Methods

In part 1, they used a correlation test to check if there’s a relationship between how far something is from the headwater and how much nitrogen ends up in the water each year.

In part 2, they used a one-way ANOVA, which is a test that compares the average nitrogen load between different source types (like urban land, fertilizer, etc.).

## b. More Information Needed

One thing that’s missing is follow-up comparisons to figure out which specific sources actually differ from each other. Without that, it’s hard to tell where the differences are coming from.

It would also help to include some summary stats—like the average and spread of nitrogen load for each source so we can better understand what’s going on beyond just the p-value.

## c. Suggestions For Rewriting

Nitrogen load in the San Joaquin River Delta appears to be related to how far the site is from the headwater. As distance increases, nitrogen load tends to increase (correlation test: r = correlation coefficient, p = 0.03, α = significance level).

Nitrogen load varied depending on the source(such as urban land, atmospheric deposition, and fertilizer). A one-way ANOVA found significant differences between sources (ANOVA: F = test statistic, df = degrees of freedom, p = 0.02, α = significance level), but more information is needed, including post-hoc comparisons and group averages, to clarify which sources are actually different from each other.

# Problem 2. Data Visualization

## a. Cleaning and Summarizing

```{r read in data}
sst <- read_csv("data/SST_update2023.csv")
```

```{r sst-cleaning}
# Clean and summarize the SST data
sst_clean <- sst |>
  mutate(date = ymd(date)) |>                          # Convert date column to Date class
  mutate(
    year = year(date),                                 # Extract year from date
    month = month(date, label = TRUE, abbr = TRUE)     # Extract month as an ordered factor
  ) |>
  filter(year >= 2018 & year <= 2023) |>               # Keep only years 2018 through 2023
  group_by(year, month) |>                             # Group by year and month
  summarize(mean_monthly_sst = mean(temp, na.rm = TRUE)) |>  # Calculate mean temp per group
  ungroup()                                            # Remove grouping

# Display a random sample of 5 rows
slice_sample(sst_clean, n = 5)

# Display structure of cleaned data
str(sst_clean)
```

## b. Visualize The Data

```{r sst-plot}
# Convert year to factor so colors work correctly
sst_clean <- sst_clean |>
  mutate(year = as.factor(year))

# Create the plot
ggplot(sst_clean, aes(x = month, y = mean_monthly_sst, group = year, color = year)) +
  geom_line(linewidth = 1) +                             # Use linewidth for lines
  geom_point(size = 2) +                                 # Add points for each value
  scale_color_viridis_d(begin = 0.1, end = 0.9, option = "C") +  # Light → dark color gradient
  labs(
    x = "Month",
    y = "Mean monthly sea surface temperature (°C)",
    color = "Year"
  ) +
  theme_minimal(base_size = 13) +                        # Clean theme
  theme(
    panel.border = element_rect(color = "black", fill = NA, linewidth = 0.8),   # Border around panel
    panel.grid = element_blank(),                        # Remove all gridlines
    legend.position = c(0.1, 0.8),                        # Legend inside top-left corner
    legend.background = element_rect(fill = "white", color = "black")  # Boxed legend
  )
```
